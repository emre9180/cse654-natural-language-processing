{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5a6c4e8f-7301-4f12-af51-aea640f142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Firstly, let's import necessary turkishnlp library to syllabicate the data '''\n",
    "\n",
    "from turkishnlp import detector\n",
    "obj = detector.TurkishNLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fa31e17e-3a88-41d9-bcf2-4a5aad05d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read wikipedia data and stoer it in the 'dataset' variable'''\n",
    "\n",
    "file_path = 'turkish_wikipedia.txt'\n",
    "\n",
    "# Read the entire file into a list where each line is an element\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5e49aecc-3859-48c7-accb-58bc9f45ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Split the data as training and test datasets '''\n",
    "\n",
    "training_data = dataset[:20000]\n",
    "test_data = dataset[20000:21000]\n",
    "\n",
    "''' Convert list of lines to a single string variable to process '''\n",
    "\n",
    "training_data = ''.join(training_data)\n",
    "test_data = ''.join(test_data)\n",
    "\n",
    "''' After that, write training and test data to txt file to see the data \n",
    "    It is important to observe what we are dealing with '''\n",
    "\n",
    "with open('training_datatest.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(training_data)\n",
    "\n",
    "# Print test_data into a file\n",
    "with open('test_data.txttest.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "693292d8-50b6-4fe6-bce6-0e3eaf0bf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This is the preprocess part. \n",
    "    1. We replace the Turkish characters by English characters to process.\n",
    "    2. We replace space characters with a token <wtspc>. We need to use the space characters as syllabels \n",
    "    3. We replace the DOT (.) characters with a token list <endsntnc>, <bgnsntnc>. I put some rules to this. If there is a number after the dot,\n",
    "do NOT put there endsntnc. Also, if there is NO another syllable after DOT, do NOT put there <bgnsntnc>\n",
    "    4. We remove all characters excluding letters, spaces and dots\n",
    "    5. We remove wikipedia links. It breaks the probability distrubution\n",
    "\n",
    "After that, we process the training and test data.\n",
    "'''\n",
    "    \n",
    "    \n",
    "def manipulate_string(input_string):\n",
    "    current_word = \"\"\n",
    "    in_title = False  # Flag to indicate if currently inside a title\n",
    "    beginFlag = False\n",
    "\n",
    "    for i, char in enumerate(input_string):\n",
    "        if char == '<':\n",
    "            in_title = True\n",
    "            continue\n",
    "        elif char == '>':\n",
    "            in_title = False\n",
    "            continue\n",
    "\n",
    "        elif char==';':\n",
    "            char = '.'\n",
    "\n",
    "        if not in_title and (char.isalpha() or char in ['.', ' ']):\n",
    "            if char.isupper():\n",
    "                char = char.lower()\n",
    "\n",
    "            if char in ['ü', 'ğ', 'ş', 'ı', 'ç', 'ö', 'ü', 'â']:\n",
    "                char = {'ü': 'u', 'ğ': 'g', 'ş': 's', 'ı': 'i', 'ç': 'c', 'ö': 'o', 'ü': 'u', 'â': 'a'}[char]\n",
    "                current_word += char\n",
    "                continue\n",
    "\n",
    "            elif char == ' ':\n",
    "                if(beginFlag):\n",
    "                    beginFlag = False\n",
    "                    continue\n",
    "                char = \" <wtspc> \"\n",
    "                current_word += char\n",
    "                continue\n",
    "            elif char == '.':\n",
    "                # Handle dot and check for the end and beginning of sentences\n",
    "                current_word += \" <endsntnc> \"\n",
    "                if input_string.index(char) < len(input_string) - 1 and (input_string[i + 1].isalpha() or input_string[i + 1] == ' '):\n",
    "                    current_word += \" <bgnsntnc> \"\n",
    "                    beginFlag = True\n",
    "                continue  \n",
    "            else:\n",
    "                current_word += char\n",
    "\n",
    "    return current_word\n",
    "\n",
    "test_data = manipulate_string(test_data)\n",
    "training_data = manipulate_string(training_data)\n",
    "\n",
    "\n",
    "'''\n",
    "    Then, we print the preprocessed strings to a file to see it.\n",
    "'''\n",
    "\n",
    "# Write training data to a file\n",
    "with open('training_data.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in training_data:\n",
    "        file.write(line)\n",
    "\n",
    "# Write test data to a file\n",
    "with open('test_data.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in test_data:\n",
    "        file.write(line)\n",
    "\n",
    "training_data_string = training_data\n",
    "test_data_string = test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "58f5efbd-b32b-41f7-b629-4dd8f21e86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Now, we come to one of the most important parts. We syllabicate strings. Convert them into strings.\n",
    "'''\n",
    "training = obj.syllabicate_sentence(training_data_string)\n",
    "test = obj.syllabicate_sentence(test_data_string)\n",
    "\n",
    "# Flatten the list of lists into a single list of strings\n",
    "training_flat = [item for sublist in training for item in sublist]\n",
    "test_flat = [item for sublist in test for item in sublist]\n",
    "\n",
    "# Join each sublist with the specified token\n",
    "result_string = ' '.join(training_flat)\n",
    "result_string_test = ' '.join(test_flat)\n",
    "\n",
    "\n",
    "'''\n",
    "    We print the probabilities to a file.\n",
    "'''\n",
    "\n",
    "# Write the result to a text file\n",
    "with open('syllabicated_result_with_tags.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(result_string)\n",
    "\n",
    "with open('syllabicated_result_with_tags_test.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(result_string_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7d17488b-3851-46f7-88a0-c1a6a7e8bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Now, we will create n-gram tables. \n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "# Convert string to a flat list of words\n",
    "tokens = result_string.split()\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "# Normalize unigram counts to probabilities\n",
    "# Create a Frequency Distribution (Unigram Table) using NLTK\n",
    "unigram_table = FreqDist(tokens)\n",
    "unigram_table_probabilities = {word: count / total_tokens for word, count in unigram_table.items()}\n",
    "\n",
    "# Calculate bigram counts and probabilities\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "bigram_table = FreqDist(bigrams_list)\n",
    "bigram_table_probabilities = {bigram: count / unigram_table[bigram[0]] for bigram, count in bigram_table.items()}\n",
    "\n",
    "# Calculate trigram counts and probabilities\n",
    "trigrams_list = list(trigrams(tokens))\n",
    "trigram_table = FreqDist(trigrams_list)\n",
    "trigram_table_probabilities = {trigram: count / bigram_table[trigram[:2]] for trigram, count in trigram_table.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "22dd731b-c9b3-46c3-b80f-29501a748d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Print the count and probability tables to a text file\n",
    "'''\n",
    "\n",
    "# Write the entire unigram table to a text file\n",
    "with open('unigram_table.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, frequency in unigram_table.items():\n",
    "        file.write(f'{word}: {frequency}\\n')\n",
    "\n",
    "# Write the entire unigram table with probabilities to a text file\n",
    "with open('unigram_table_probabilities.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, probability in unigram_table_probabilities.items():\n",
    "        file.write(f'{word}: {probability}\\n')\n",
    "        \n",
    "# Write the entire unigram table to a text file\n",
    "with open('bigram_table.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, frequency in bigram_table.items():\n",
    "        file.write(f'{word}: {frequency}\\n')\n",
    "\n",
    "# Write the entire bigram table with probabilities to a text file\n",
    "with open('bigram_table_probabilities.txt', 'w', encoding='utf-8') as file:\n",
    "    for bigram, probability in bigram_table_probabilities.items():\n",
    "        file.write(f'{bigram}: {probability}\\n')\n",
    "\n",
    "# Write the entire unigram table to a text file\n",
    "with open('trigram_table.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, frequency in trigram_table.items():\n",
    "        file.write(f'{word}: {frequency}\\n')\n",
    "\n",
    "# Write the entire trigram table with probabilities to a text file\n",
    "with open('trigram_table_probabilities.txt', 'w', encoding='utf-8') as file:\n",
    "    for trigram, probability in trigram_table_probabilities.items():\n",
    "        file.write(f'{trigram}: {probability}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "21af2b6e-435b-46b3-a99c-575748b9e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Sort the probabililites to use to generate random sentences. Write them into a file \n",
    "'''\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def sort_and_write_probabilities(probabilities, file_path):\n",
    "    # Sort probabilities by descending order\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    # Write sorted probabilities to a file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for item in sorted_probabilities:\n",
    "            file.write(f'{item[0]}: {item[1]}\\n')\n",
    "    return sorted_probabilities\n",
    "\n",
    "sorted_unigram_table = sort_and_write_probabilities(unigram_table_probabilities, 'sorted_unigram_probabilities.txt')\n",
    "sorted_bigram_table = sort_and_write_probabilities(bigram_table_probabilities, 'sorted_bigram_probabilities.txt')\n",
    "sorted_trigram_table = sort_and_write_probabilities(trigram_table_probabilities, 'sorted_trigram_probabilities.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "37ea41ca-ba68-43b1-bf96-bdad677f85e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity: 135.12665729754758\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Calculate unigram probability table\n",
    "'''\n",
    "\n",
    "from nltk import FreqDist\n",
    "import math\n",
    "\n",
    "def calculate_unigram_perplexity(test_tokens, unigram_table):\n",
    "    N = len(test_tokens)\n",
    "    log_sum = 0.0\n",
    "\n",
    "    for i, token in enumerate(test_tokens):\n",
    "        # Check if the token exists in the unigram table\n",
    "        if token in unigram_table:\n",
    "            conditional_probability = unigram_table[token]\n",
    "            log_sum += math.log(conditional_probability)\n",
    "        else:\n",
    "            last_tuple = sorted_unigram_table[-1]  # Get the last tuple in the list\n",
    "            last_key = last_tuple[0]              # The bigram (key)\n",
    "            last_value = last_tuple[1]            # The probability (value)\n",
    "            conditional_probability = last_value\n",
    "            log_sum += math.log(conditional_probability)\n",
    "\n",
    "    perplexity = math.exp(-log_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "tokens = result_string_test.split()\n",
    "perplexity_unigram = calculate_unigram_perplexity(tokens, unigram_table_probabilities)\n",
    "print(f'Unigram Perplexity: {perplexity_unigram}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e52db108-8b4f-4c6c-9138-d6aca44a0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity: 24.87655422929261\n",
      "Trigram Perplexity: 12.694723409536026\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import math\n",
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "def calculate_bigram_perplexity(test_tokens, bigram_table, unigram_table):\n",
    "    N = len(test_tokens)\n",
    "    log_sum = 0.0\n",
    "\n",
    "    bigrams_list = list(bigrams(test_tokens))\n",
    "\n",
    "    for bigram in bigrams_list:\n",
    "        # Check if the bigram exists in the bigram table\n",
    "        if bigram in bigram_table:\n",
    "            conditional_probability = bigram_table[bigram]\n",
    "        elif bigram[1] in unigram_table:\n",
    "            # Use backoff to unigram\n",
    "            conditional_probability = unigram_table[bigram[1]]\n",
    "        else:\n",
    "            # If bigram and unigram are not found, set conditional probability to a small value\n",
    "            last_tuple = sorted_bigram_table[-1]  # Get the last tuple in the list\n",
    "            last_key = last_tuple[0]              # The bigram (key)\n",
    "            last_value = last_tuple[1]            # The probability (value)\n",
    "            conditional_probability = last_value\n",
    "\n",
    "        log_sum += math.log(conditional_probability)\n",
    "\n",
    "    perplexity = math.exp(-log_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "def calculate_trigram_perplexity(test_tokens, trigram_table, bigram_table, unigram_table):\n",
    "    N = len(test_tokens)\n",
    "    log_sum = 0.0\n",
    "\n",
    "    trigrams_list = list(trigrams(test_tokens))\n",
    "\n",
    "    for trigram in trigrams_list:\n",
    "        # Check if the trigram exists in the trigram table\n",
    "        if trigram in trigram_table:\n",
    "            conditional_probability = trigram_table[trigram]\n",
    "        elif trigram[1:] in bigram_table:\n",
    "            # Use backoff to bigram\n",
    "            conditional_probability = bigram_table[trigram[1:]]\n",
    "        elif trigram[2] in unigram_table:\n",
    "            # Use backoff to unigram\n",
    "            conditional_probability = unigram_table[trigram[2]]\n",
    "        else:\n",
    "            # If trigram, bigram, and unigram are not found, set conditional probability to a small value\n",
    "            last_tuple = sorted_trigram_table[-1]  # Get the last tuple in the list\n",
    "            last_key = last_tuple[0]              # The bigram (key)\n",
    "            last_value = last_tuple[1]            # The probability (value)\n",
    "            conditional_probability = last_value\n",
    "\n",
    "        log_sum += math.log(conditional_probability)\n",
    "\n",
    "    perplexity = math.exp(-log_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "tokens = result_string_test.split()\n",
    "\n",
    "perplexity_bigram = calculate_bigram_perplexity(tokens, bigram_table_probabilities, unigram_table_probabilities)\n",
    "print(f'Bigram Perplexity: {perplexity_bigram}')\n",
    "\n",
    "perplexity_trigram = calculate_trigram_perplexity(tokens, trigram_table_probabilities, bigram_table_probabilities, unigram_table_probabilities)\n",
    "print(f'Trigram Perplexity: {perplexity_trigram}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "8cb0dd5c-8f9d-46da-9df2-7e2a6c7db318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram random sentence is:  Cikti.Bu duzen gostivar kitaplaridirlar. \n",
      "\n",
      "\n",
      "bigram random sentence is:  Anaya ozelyetirildisininmislarlamasininmislar. \n",
      "\n",
      "\n",
      "unigram random sentence is:  Lala.lerilalalelelale.la ri lela.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Generate random sentences. We use Shennon Method here. And postprocess the result to put spaces and dots to appropriate places\n",
    "    in the sentence.\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "def unify_syllables(syllables):\n",
    "    result = []\n",
    "    upperCaseFlag = False\n",
    "    for syllable in syllables:\n",
    "        if 'endsntnc' in syllable:\n",
    "            result.append('.')\n",
    "            upperCaseFlag = False\n",
    "        elif 'wtspc' in syllable:\n",
    "            result.append(' ')\n",
    "            upperCaseFlag = False\n",
    "        elif 'bgnsntnc' in syllable:\n",
    "            upperCaseFlag = True\n",
    "        else:\n",
    "            if upperCaseFlag:\n",
    "                result.append(syllable[0].upper() + syllable[1:])  # Make only the first letter uppercase\n",
    "            else:\n",
    "                result.append(syllable)\n",
    "            upperCaseFlag = False\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "    \n",
    "def top_words(words_with_probs, top_n=5):\n",
    "        # Sort words based on their probabilities and return the top N words\n",
    "        sorted_words = sorted(words_with_probs, key=lambda item: item[1], reverse=True)\n",
    "        return [word for word, _ in sorted_words[:top_n]]\n",
    "\n",
    "def generate_random_sentence_unigram(unigram_table, starting_syllable, min_syllables=20):\n",
    "    sentence = [starting_syllable]  # Start the sentence with the given syllable\n",
    "\n",
    "    # Sort the unigrams based on probability and get the top 5\n",
    "    top_unigrams = sorted(unigram_table.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    # Extract just the words from the top unigrams\n",
    "    top_words = [word for word, _ in top_unigrams]\n",
    "\n",
    "    while True:\n",
    "        next_word = random.choice(top_words)\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        if len(sentence) >= min_syllables and next_word == 'endsntnc':\n",
    "            break\n",
    "\n",
    "    return unify_syllables(sentence)\n",
    "    \n",
    "\n",
    "def generate_random_sentence_starting_with(n_table, unigram_table, bigram_table, trigram_table, n, starting_syllable, min_syllables=20):\n",
    "    sentence = []\n",
    "\n",
    "    # Find n-grams that start with the given syllable\n",
    "    initial_ngrams = [ngram for ngram in n_table.keys() if ngram[0] == starting_syllable]\n",
    "\n",
    "    if not initial_ngrams:\n",
    "        return \"No sentence found with the given starting syllable.\"\n",
    "\n",
    "    # Choose a random n-gram from the ones that start with the given syllable\n",
    "    initial_ngram = random.choice(initial_ngrams)\n",
    "    sentence.extend(initial_ngram[:-1])\n",
    "\n",
    "    while True:\n",
    "        # Generate the next word based on the last (n-1) words\n",
    "        last_ngram = tuple(sentence[-(n - 1):])\n",
    "        possible_next_words_with_probs = [(ngram[-1], n_table[ngram]) for ngram in n_table.keys() if ngram[:-1] == last_ngram]\n",
    "\n",
    "        if not possible_next_words_with_probs and n > 2:  # Try bigram if trigram fails\n",
    "            last_bigram = tuple(sentence[-(2 - 1):])\n",
    "            possible_next_words_with_probs = [(ngram[-1], bigram_table[ngram]) for ngram in bigram_table.keys() if ngram[:-1] == last_bigram]\n",
    "\n",
    "        if not possible_next_words_with_probs and n > 1:  # Try unigram if bigram fails\n",
    "            top_unigrams = sorted(unigram_table.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "            possible_next_words_with_probs = top_unigrams\n",
    "\n",
    "        if possible_next_words_with_probs:\n",
    "            top_words_list = top_words(possible_next_words_with_probs, top_n=5)\n",
    "            next_word = random.choice(top_words_list) if top_words_list else None\n",
    "            if next_word:\n",
    "                sentence.append(next_word)\n",
    "                if len(sentence) >= min_syllables and next_word == 'endsntnc':\n",
    "                    break\n",
    "        else:\n",
    "            # If no valid next words are found even after backoff, break the loop\n",
    "            break\n",
    "\n",
    "    return unify_syllables(sentence)\n",
    "\n",
    "# Example usage\n",
    "starting_syllable = \"bgnsntnc\"  # Replace with the actual starting syllable\n",
    "generated_sentence = generate_random_sentence_starting_with(trigram_table_probabilities, unigram_table_probabilities, bigram_table_probabilities, trigram_table_probabilities, 3, starting_syllable)\n",
    "generated_sentence2 = generate_random_sentence_starting_with(bigram_table_probabilities, unigram_table_probabilities, bigram_table_probabilities, trigram_table_probabilities, 2, starting_syllable)\n",
    "\n",
    "print(\"trigram random sentence is: \", generated_sentence, '\\n\\n')\n",
    "print(\"bigram random sentence is: \", generated_sentence2, '\\n\\n')\n",
    "\n",
    "\n",
    "generated_sentence3 = generate_random_sentence_unigram(unigram_table_probabilities, starting_syllable, 20)\n",
    "print(\"unigram random sentence is: \", generated_sentence3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06994247-134e-4de6-8cde-a05c3c5bd2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fa0d3-09ac-4538-9bfb-9abf29c32fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
